{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import spacy\n",
    "import tensorflow\n",
    "import keras\n",
    "# from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "# from watson_developer_cloud.natural_language_understanding_v1 \\\n",
    "#   import Features, KeywordsOptions, EntitiesOptions, SemanticRolesOptions\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "import inflection\n",
    "import language_check # grammar checker\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# set grammar checker\n",
    "grammar_tool = language_check.LanguageTool('en-US')\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "Apple ORG\n",
      "US GPE\n",
      "Apple ORG\n",
      "PetroChina ORG\n",
      "Chinese NORP\n",
      "Standard Oil ORG\n",
      "Apple ORG\n",
      "Apple ORG\n",
      "Apple ORG\n",
      "the Apple Watch ORG\n",
      "Apple ORG\n",
      "Nokia ORG\n",
      "Apple ORG\n",
      "Cisco GPE\n",
      "Microsoft ORG\n",
      "Cisco ORG\n",
      "Apple ORG\n",
      "Apple ORG\n",
      "Apple ORG\n",
      "Steve Jobs PERSON\n",
      "Jobs PERSON\n",
      "Apple Computer ORG\n",
      "iPhone ORG\n",
      "Apple ORG\n",
      "iPhone ORG\n",
      "Apple ORG\n",
      "Macs ORG\n",
      "iPads ORG\n",
      "Apple ORG\n",
      "Google Maps LOC\n",
      "YouTube GPE\n",
      "Apple ORG\n",
      "iCloud ORG\n",
      "Apple ORG\n",
      "iPhone ORG\n",
      "iPad ORG\n",
      "Mac ORG\n",
      "Apple ORG\n",
      "Samsung ORG\n",
      "LG ORG\n",
      "Huawei ORG\n",
      "Xiaomi ORG\n",
      "Amazon ORG\n",
      "Google ORG\n",
      "Amazon Web Services ORG\n",
      "Google ORG\n",
      "Bill Gates PERSON\n",
      "Microsoft ORG\n",
      "Apple ORG\n",
      "Apple ORG\n",
      "IBM ORG\n",
      "Exxon ORG\n",
      "Apple ORG\n",
      "iPhone ORG\n",
      "the United States GPE\n",
      "Apple ORG\n",
      "Apple ORG\n",
      "America GPE\n"
     ]
    }
   ],
   "source": [
    "with open('article.txt', 'r') as file:\n",
    "    doc = nlp(file.read())\n",
    "\n",
    "entities = doc.ents\n",
    "sentences = doc.sents\n",
    "exceptList = ['DATE', 'MONEY', 'TIME', 'ORDINAL', 'QUANTITY', 'CARDINAL', 'PERCENT', 'ORDINAL']\n",
    "for ent in entities:\n",
    "    if ent.label_ not in exceptList:\n",
    "        print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load credentials from file (Store credentials in json format)\n",
    "# with open('credentials.json') as f:\n",
    "#     data = json.load(f)\n",
    "# url = data[\"url\"]\n",
    "# username = data[\"username\"]\n",
    "# password = data[\"password\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "#   username=username,\n",
    "#   password=password,\n",
    "#   version='2018-03-16')\n",
    "\n",
    "# response = natural_language_understanding.analyze(\n",
    "#     url='https://www.nytimes.com/2018/07/16/opinion/trump-and-putin-vs-america.html?action=click&pgtype=Homepage&clickSource=story-heading&module=opinion-c-col-left-region&region=opinion-c-col-left-region&WT.nav=opinion-c-col-left-region',\n",
    "#     language='en',\n",
    "#     features=Features(\n",
    "#         keywords=KeywordsOptions(\n",
    "#             sentiment=False,\n",
    "#             emotion=False,\n",
    "#             limit=20),\n",
    "#     entities=EntitiesOptions(\n",
    "#         sentiment=False,\n",
    "#             emotion=False,\n",
    "#             limit=50),\n",
    "#     semantic_roles=SemanticRolesOptions()\n",
    "#   ))\n",
    "# entities = response['entities']\n",
    "# keywords = response['keywords']\n",
    "# semantic = response['semantic_roles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1\n",
    "# Extract keywords and entities\n",
    "# define type of words\n",
    "# create questions\n",
    "def Q1(x):\n",
    "    return {\n",
    "        'PERSON': \"Who is \",\n",
    "        'GPE': \"Where is \"\n",
    "    }.get(x, \"What is \")\n",
    "\n",
    "keyEntities = set()\n",
    "duplicates = set()\n",
    "\n",
    "with open(\"Questions1.txt\", \"w\") as file:\n",
    "    for ent in entities:\n",
    "        if ent.label_ not in exceptList and ent.text not in duplicates:\n",
    "            # make question\n",
    "            text = Q1(ent.label_) + ent.text + \"?\"\n",
    "            # check and fix grammar of question\n",
    "            matches = grammar_tool.check(text)\n",
    "            correct_text = language_check.correct(text, matches)\n",
    "            # write to file and add to unique set\n",
    "            file.write(\"%s\\n\" % correct_text)\n",
    "            keyEntities.add(ent)\n",
    "            duplicates.add(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs\n",
      "America\n",
      "Steve Jobs\n",
      "YouTube\n",
      "Bill Gates\n",
      "Huawei\n",
      "Google\n",
      "Cisco\n",
      "Microsoft\n",
      "Samsung\n",
      "Google Maps\n",
      "Xiaomi\n",
      "US\n",
      "Chinese\n",
      "Exxon\n",
      "LG\n",
      "Standard Oil\n",
      "iCloud\n",
      "Nokia\n",
      "PetroChina\n",
      "Amazon Web Services\n",
      "Macs\n",
      "iPads\n",
      "iPhone\n",
      "Amazon\n",
      "IBM\n",
      "the United States\n",
      "the Apple Watch\n",
      "iPad\n",
      "Apple Computer\n",
      "Apple\n",
      "Mac\n"
     ]
    }
   ],
   "source": [
    "for a in keyEntities:\n",
    "    print(a.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2\n",
    "# Extract keywords and entities\n",
    "# define plurality of keywords using nltk\n",
    "# create question What are? Who are? \n",
    "# TODO. How to determine plurality of word?\n",
    "# TODO. If condition does not suffice, what else?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 3\n",
    "\n",
    "duplicates = set()\n",
    "\n",
    "with open(\"refined_named_entities.txt\", 'r') as file: # create tokens from known named entities\n",
    "    doc = nlp(file.read()) \n",
    "# for t in doc.ents:\n",
    "#     print(t.text)\n",
    "    \n",
    "with open(\"Questions3.txt\", \"w\") as file: # create questions based on similarity between 'entities' and 'known named entities'\n",
    "    for key in keyEntities:\n",
    "        for token in doc.ents:\n",
    "            tokenSimilarity = token.similarity(key)\n",
    "            if tokenSimilarity > 0.6 and tokenSimilarity != 1 and token.text not in duplicates and len(token.text) > 2:\n",
    "                file.write(\"What is the difference between %s and %s?\\n\" % (token.text, key.text))\n",
    "                duplicates.add(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sents:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4\n",
    "# Why (factual statement)?\n",
    "# TODO. How to determine which sentences are important enough to negate? TEXTSUM, sumy NOTWORKING\n",
    "\n",
    "# Question 5\n",
    "# What if (negated statements)?\n",
    "# TODO. How to negate sentences? add not, find antonym\n",
    "# TODO. How to determine which sentences are important enough to negate? summarize? gensim, pyteaser, pytextrank, TEXTSUM, sumy\n",
    "\n",
    "Q4 = []\n",
    "Q5 = []\n",
    "\n",
    "listOfPlurals = [\"they\", \"some\", \"most\", \"we\"]\n",
    "translator = str.maketrans('', '', string.punctuation) #strip punctuations\n",
    "\n",
    "for sentence in semantic:\n",
    "    if len(sentence) == 4:\n",
    "        verb = sentence['action']['normalized']\n",
    "        subj = sentence['subject']['text'].capitalize()\n",
    "        obj = sentence['object']['text']\n",
    "        if verb is not \"s\" and verb != \"be\":\n",
    "            plurality = subj is not inflection.singularize(subj) or subj in listOfPlurals\n",
    "            if plurality:\n",
    "                Q4.append(\"Why do \" + subj + \" \" + verb + \" \" + obj + \"?\")\n",
    "            else:\n",
    "                Q4.append(\"Why does \" + subj + \" \" + verb + \" \" + obj + \"?\")\n",
    "            Q5.append(\"What if \" + subj + \" did not \" + verb + \" \" + obj + \"?\")\n",
    "        elif verb == \"be\":\n",
    "            plurality = subj is not inflection.singularize(subj) or subj in listOfPlurals\n",
    "            if plurality:\n",
    "                Q4.append(\"Why are \" + subj + \" \" + obj + \"?\")\n",
    "                Q5.append(\"What if \" + subj + \" were not \" + obj + \"?\")\n",
    "            else:\n",
    "                Q4.append(\"Why is \" + subj + \" \" + obj + \"?\")\n",
    "                Q5.append(\"What if \" + subj + \" was not \" + obj + \"?\")\n",
    "        \n",
    "with open(\"Questions4.txt\", \"w\", encoding='UTF-8') as file:\n",
    "    for q in Q4:\n",
    "        grammarCheckAndWrite(q, file)\n",
    "\n",
    "with open(\"Questions5.txt\", \"w\", encoding='UTF-8') as file:\n",
    "    for q in Q5:\n",
    "        grammarCheckAndWrite(q, file)\n",
    "\n",
    "# TODO: Check relevance to text\n",
    "# TODO: lets do grammar check first and delete or fix the grammar before comparing relevance to text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grammarCheckAndWrite(question, file):\n",
    "    matches = grammar_tool.check(question)\n",
    "    corrected_q = language_check.correct(question, matches)\n",
    "    file.write(\"%s\\n\" % corrected_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
