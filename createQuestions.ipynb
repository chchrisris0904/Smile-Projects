{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import spacy\n",
    "import tensorflow\n",
    "import keras\n",
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "from watson_developer_cloud.natural_language_understanding_v1 \\\n",
    "  import Features, KeywordsOptions, EntitiesOptions, SemanticRolesOptions\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "import inflection\n",
    "\n",
    "import en_core_web_lg\n",
    "import language_check\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load credentials from file (Store credentials in json format)\n",
    "with open('credentials.json') as f:\n",
    "    data = json.load(f)\n",
    "url = data[\"url\"]\n",
    "username = data[\"username\"]\n",
    "password = data[\"password\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set grammar checker\n",
    "grammar_tool = language_check.LanguageTool('en-US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "  username=username,\n",
    "  password=password,\n",
    "  version='2018-03-16')\n",
    "\n",
    "response = natural_language_understanding.analyze(\n",
    "    url='https://www.nytimes.com/2018/07/16/opinion/trump-and-putin-vs-america.html?action=click&pgtype=Homepage&clickSource=story-heading&module=opinion-c-col-left-region&region=opinion-c-col-left-region&WT.nav=opinion-c-col-left-region',\n",
    "    language='en',\n",
    "    features=Features(\n",
    "        keywords=KeywordsOptions(\n",
    "            sentiment=False,\n",
    "            emotion=False,\n",
    "            limit=20),\n",
    "    entities=EntitiesOptions(\n",
    "        sentiment=False,\n",
    "            emotion=False,\n",
    "            limit=50),\n",
    "    semantic_roles=SemanticRolesOptions()\n",
    "  ))\n",
    "entities = response['entities']\n",
    "keywords = response['keywords']\n",
    "semantic = response['semantic_roles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1\n",
    "# Extract keywords and entities\n",
    "# define type of words\n",
    "# create questions\n",
    "def Q1(x):\n",
    "    return {\n",
    "        'Person': \"Who is \",\n",
    "        'Location': \"Where is \"\n",
    "    }.get(x, \"What is \")\n",
    "\n",
    "with open(\"Questions1.txt\", \"w\") as file:\n",
    "    for en in entities:\n",
    "        text = Q1(en['type']) + en['text'] + \"?\"\n",
    "        matches = grammar_tool.check(text)\n",
    "        correct_text = language_check.correct(text, matches)\n",
    "        file.write(\"%s\\n\" % correct_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2\n",
    "# Extract keywords and entities\n",
    "# define plurality of keywords using nltk\n",
    "# create question What are? Who are? \n",
    "# TODO. How to determine plurality of word?\n",
    "# TODO. If condition does not suffice, what else?\n",
    "\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def isplural(word):\n",
    "    lemma = wnl.lemmatize(word, 'n')\n",
    "    plural = True if word is not lemma else False\n",
    "    return plural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3\n",
    "# How would you compare A to B?\n",
    "# For each keyword, look for other words that have same type and relatively high relevance in DB of news articles???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enjoy\n",
      "enjoy play\n",
      "reinforce\n",
      "have\n",
      "to make\n",
      "engage\n",
      "violate\n",
      "to ``preserve\n",
      "vacate\n",
      "will be\n",
      "issue\n",
      "sit\n",
      "have\n",
      "see\n",
      "like\n",
      "add\n",
      "will\n",
      "ll bet\n",
      "get\n",
      "be ask\n",
      "believe\n",
      "throw\n",
      "actually say\n",
      "do\n",
      "to blame\n",
      "make\n",
      "say\n",
      "keep\n",
      "dis\n",
      "blame\n",
      "try\n",
      "try to defeat\n",
      "make\n",
      "endorse\n",
      "exceed\n"
     ]
    }
   ],
   "source": [
    "# Question 4\n",
    "# Why (factual statement)?\n",
    "# TODO. How to determine which sentences are important enough to negate? TEXTSUM, sumy NOTWORKING\n",
    "\n",
    "# Question 5\n",
    "# What if (negated statements)?\n",
    "# TODO. How to negate sentences? add not, find antonym\n",
    "# TODO. How to determine which sentences are important enough to negate? summarize? gensim, pyteaser, pytextrank, TEXTSUM, sumy\n",
    "\n",
    "\n",
    "# LANGUAGE = \"english\"\n",
    "# SENTENCES_COUNT = 6\n",
    "# if __name__ == \"__main__\":\n",
    "#     #url = \"https://en.wikipedia.org/wiki/Steven_Spielberg\"\n",
    "#     #parser = HtmlParser.from_url(url, Tokenizer(LANGUAGE))\n",
    "#     # or for plain text files\n",
    "#     parser = PlaintextParser.from_file(\"document.txt\", Tokenizer(LANGUAGE))\n",
    "#     stemmer = Stemmer(LANGUAGE)\n",
    "\n",
    "#     summarizer = Summarizer(stemmer)\n",
    "#     summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "#     for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "#         result = natural_language_understanding.analyze(text=str(sentence), features=Features(semantic_roles=SemanticRolesOptions()))\n",
    "#         #print(json.dumps(result, indent=2))\n",
    "#         for sentence in result['semantic_roles']:\n",
    "#             print(len(sentence))\n",
    "#             if len(sentence) == 4:\n",
    "#                 print(\"Why Does\", sentence['subject']['text'], verb, obj)\n",
    "#                 print(\"What if\", sentence['subject']['text'], \"did not\",verb, obj)\n",
    "\n",
    "\n",
    "\n",
    "Q4 = []\n",
    "Q5 = []\n",
    "\n",
    "listOfPlurals = [\"they\", \"some\", \"most\", \"we\"]\n",
    "\n",
    "for sentence in semantic:\n",
    "    if len(sentence) == 4:\n",
    "        verb = sentence['action']['normalized']\n",
    "        subj = sentence['subject']['text'].lower()\n",
    "        obj = sentence['object']['text']\n",
    "        if verb is not \"s\" and verb != \"be\":\n",
    "            plurality = subj is not inflection.singularize(subj) or subj in listOfPlurals\n",
    "            print(verb)\n",
    "            if plurality:\n",
    "                Q4.append(\"Why do \" + subj + \" \" + verb + \" \" + obj + \"?\")\n",
    "            else:\n",
    "                Q4.append(\"Why does \" + subj + \" \" + verb + \" \" + obj + \"?\")\n",
    "            Q5.append(\"What if \" + subj + \" did not \" + verb + \" \" + obj + \"?\")\n",
    "        elif verb == \"be\":\n",
    "            plurality = subj is not inflection.singularize(subj) or subj in listOfPlurals\n",
    "            if plurality:\n",
    "                Q4.append(\"Why are \" + subj + \" \" + obj + \"?\")\n",
    "                Q5.append(\"What if \" + subj + \" were not \" + obj + \"?\")\n",
    "            else:\n",
    "                Q4.append(\"Why is \" + subj + \" \" + obj + \"?\")\n",
    "                Q5.append(\"What if \" + subj + \" was not \" + obj + \"?\")\n",
    "        \n",
    "with open(\"Questions4.txt\", \"w\", encoding='UTF-8') as file:\n",
    "    for q in Q4:\n",
    "        matches = grammar_tool.check(q)\n",
    "        corrected_q = language_check.correct(q, matches)\n",
    "        file.write(\"%s\\n\" % corrected_q)\n",
    "with open(\"Questions5.txt\", \"w\", encoding='UTF-8') as file:\n",
    "    for q in Q5:\n",
    "        matches = grammar_tool.check(q)\n",
    "        corrected_q = language_check.correct(q, matches)\n",
    "        file.write(\"%s\\n\" % corrected_q)\n",
    "\n",
    "# TODO: Check relevance to text\n",
    "# TODO: lets do grammar check first and delete or fix the grammar before comparing relevance to text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('fetch-group-activity.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from summa import summarizer\n",
    "with open('document.txt', 'r') as myfile:\n",
    "    text = myfile.read().replace('\\n', '')\n",
    "\n",
    "print(summarizer.summarize(text, words=50, split=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Google 1.0\n",
      "Google Facebook 0.73055875\n",
      "Google Honda 0.2760702\n",
      "Facebook Google 0.73055875\n",
      "Facebook Facebook 1.0\n",
      "Facebook Honda 0.22290869\n",
      "Honda Google 0.2760702\n",
      "Honda Facebook 0.22290869\n",
      "Honda Honda 1.0\n"
     ]
    }
   ],
   "source": [
    "  # make sure to use larger model!\n",
    "tokens = nlp(u'Google Facebook Honda')\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(semantic, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
