{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import spacy\n",
    "import tensorflow\n",
    "import keras\n",
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "from watson_developer_cloud.natural_language_understanding_v1 \\\n",
    "  import Features, KeywordsOptions, EntitiesOptions, SemanticRolesOptions\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "# from __future__ import absolute_import\n",
    "# from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# from sumy.parsers.html import HtmlParser\n",
    "# from sumy.parsers.plaintext import PlaintextParser\n",
    "# from sumy.nlp.tokenizers import Tokenizer\n",
    "# from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "# from sumy.nlp.stemmers import Stemmer\n",
    "# from sumy.utils import get_stop_words\n",
    "\n",
    "import inflection\n",
    "import language_check\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load credentials from file (Store credentials in json format)\n",
    "with open('credentials.json') as f:\n",
    "    data = json.load(f)\n",
    "url = data[\"url\"]\n",
    "username = data[\"username\"]\n",
    "password = data[\"password\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set grammar checker\n",
    "grammar_tool = language_check.LanguageTool('en-US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "  username=username,\n",
    "  password=password,\n",
    "  version='2018-03-16')\n",
    "\n",
    "response = natural_language_understanding.analyze(\n",
    "    url='https://www.nytimes.com/2018/07/16/opinion/trump-and-putin-vs-america.html?action=click&pgtype=Homepage&clickSource=story-heading&module=opinion-c-col-left-region&region=opinion-c-col-left-region&WT.nav=opinion-c-col-left-region',\n",
    "    language='en',\n",
    "    features=Features(\n",
    "        keywords=KeywordsOptions(\n",
    "            sentiment=False,\n",
    "            emotion=False,\n",
    "            limit=20),\n",
    "    entities=EntitiesOptions(\n",
    "        sentiment=False,\n",
    "            emotion=False,\n",
    "            limit=50),\n",
    "    semantic_roles=SemanticRolesOptions()\n",
    "  ))\n",
    "entities = response['entities']\n",
    "keywords = response['keywords']\n",
    "semantic = response['semantic_roles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1\n",
    "# Extract keywords and entities\n",
    "# define type of words\n",
    "# create questions\n",
    "def Q1(x):\n",
    "    return {\n",
    "        'Person': \"Who is \",\n",
    "        'Location': \"Where is \"\n",
    "    }.get(x, \"What is \")\n",
    "\n",
    "with open(\"Questions1.txt\", \"w\") as file:\n",
    "    for en in entities:\n",
    "        text = Q1(en['type']) + en['text'] + \"?\"\n",
    "        matches = grammar_tool.check(text)\n",
    "        correct_text = language_check.correct(text, matches)\n",
    "        file.write(\"%s\\n\" % correct_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2\n",
    "# Extract keywords and entities\n",
    "# define plurality of keywords using nltk\n",
    "# create question What are? Who are? \n",
    "# TODO. How to determine plurality of word?\n",
    "# TODO. If condition does not suffice, what else?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4\n",
    "# Why (factual statement)?\n",
    "# TODO. How to determine which sentences are important enough to negate? TEXTSUM, sumy NOTWORKING\n",
    "\n",
    "# Question 5\n",
    "# What if (negated statements)?\n",
    "# TODO. How to negate sentences? add not, find antonym\n",
    "# TODO. How to determine which sentences are important enough to negate? summarize? gensim, pyteaser, pytextrank, TEXTSUM, sumy\n",
    "\n",
    "Q4 = []\n",
    "Q5 = []\n",
    "\n",
    "listOfPlurals = [\"they\", \"some\", \"most\", \"we\"]\n",
    "translator = str.maketrans('', '', string.punctuation) #strip punctuations\n",
    "\n",
    "for sentence in semantic:\n",
    "    if len(sentence) == 4:\n",
    "        verb = sentence['action']['normalized'] #strip punctuations\n",
    "        subj = sentence['subject']['text'].capitalize()\n",
    "        obj = sentence['object']['text']\n",
    "        if verb is not \"s\" and verb != \"be\":\n",
    "            plurality = subj is not inflection.singularize(subj) or subj in listOfPlurals\n",
    "            if plurality:\n",
    "                Q4.append(\"Why do \" + subj + \" \" + verb + \" \" + obj + \"?\")\n",
    "            else:\n",
    "                Q4.append(\"Why does \" + subj + \" \" + verb + \" \" + obj + \"?\")\n",
    "            Q5.append(\"What if \" + subj + \" did not \" + verb + \" \" + obj + \"?\")\n",
    "        elif verb == \"be\":\n",
    "            plurality = subj is not inflection.singularize(subj) or subj in listOfPlurals\n",
    "            if plurality:\n",
    "                Q4.append(\"Why are \" + subj + \" \" + obj + \"?\")\n",
    "                Q5.append(\"What if \" + subj + \" were not \" + obj + \"?\")\n",
    "            else:\n",
    "                Q4.append(\"Why is \" + subj + \" \" + obj + \"?\")\n",
    "                Q5.append(\"What if \" + subj + \" was not \" + obj + \"?\")\n",
    "        \n",
    "with open(\"Questions4.txt\", \"w\", encoding='UTF-8') as file:\n",
    "    for q in Q4:\n",
    "        matches = grammar_tool.check(q)\n",
    "        corrected_q = language_check.correct(q, matches)\n",
    "        file.write(\"%s\\n\" % corrected_q)\n",
    "with open(\"Questions5.txt\", \"w\", encoding='UTF-8') as file:\n",
    "    for q in Q5:\n",
    "        matches = grammar_tool.check(q)\n",
    "        corrected_q = language_check.correct(q, matches)\n",
    "        file.write(\"%s\\n\" % corrected_q)\n",
    "\n",
    "# TODO: Check relevance to text\n",
    "# TODO: lets do grammar check first and delete or fix the grammar before comparing relevance to text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to use larger model!\n",
    "exceptList = ['DATE', 'MONEY', 'TIME', 'ORDINAL', 'QUANTITY', 'CARDINAL', 'PERCENT']\n",
    "with open('article.txt', 'r') as file:\n",
    "    textQ3 = file.readlines()\n",
    "\n",
    "entities_seen = set()\n",
    "with open('named_entities.txt', 'w') as file:\n",
    "    for line in textQ3:\n",
    "        doc = nlp(line)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ not in exceptList:\n",
    "                if ent.text.title() not in entities_seen: # not a duplicate\n",
    "                    file.write(str(ent.text.title()) + '\\n')\n",
    "                    entities_seen.add(ent.text.title())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyString = \"\"\n",
    "for key in keywords:\n",
    "    keyString += key['text'].title()\n",
    "keytokens = nlp(keyString)\n",
    "\n",
    "with open('named_entities.txt', 'r') as file:\n",
    "    tokens = nlp(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup = set()\n",
    "with open(\"Questions3.txt\", \"w\") as file:\n",
    "    for key in keytokens:\n",
    "        for token in tokens:\n",
    "            tokenSimilarity = token.similarity(key)\n",
    "            if tokenSimilarity > 0.6 and tokenSimilarity != 1 and token.text not in dup and len(token.text) > 2:\n",
    "                file.write(\"What is the difference between %s and %s?\\n\" % (token.text, key.text))\n",
    "                dup.add(token.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
