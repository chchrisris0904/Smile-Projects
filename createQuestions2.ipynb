{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import spacy\n",
    "import tensorflow\n",
    "import keras\n",
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "from watson_developer_cloud.natural_language_understanding_v1   import Features, KeywordsOptions, EntitiesOptions, SemanticRolesOptions\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# from __future__ import absolute_import\n",
    "# from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# from sumy.parsers.html import HtmlParser\n",
    "# from sumy.parsers.plaintext import PlaintextParser\n",
    "# from sumy.nlp.tokenizers import Tokenizer\n",
    "# from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "# from sumy.nlp.stemmers import Stemmer\n",
    "# from sumy.utils import get_stop_words\n",
    "\n",
    "import inflection\n",
    "import en_core_web_lg\n",
    "import language_check\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# In[2]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Load credentials from file (Store credentials in json format)\n",
    "with open('credentials.json') as f:\n",
    "    data = json.load(f)\n",
    "url = data[\"url\"]\n",
    "username = data[\"username\"]\n",
    "password = data[\"password\"]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[3]:\n",
    "\n",
    "\n",
    "# set grammar checker\n",
    "grammar_tool = language_check.LanguageTool('en-US')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('article.txt', 'r') as file:\n",
    "    ttt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[4]:\n",
    "\n",
    "\n",
    "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "  username=username,\n",
    "  password=password,\n",
    "  version='2018-03-16')\n",
    "\n",
    "response = natural_language_understanding.analyze(\n",
    "    text=ttt,\n",
    "    language='en',\n",
    "    features=Features(\n",
    "        keywords=KeywordsOptions(\n",
    "            sentiment=False,\n",
    "            emotion=False,\n",
    "            limit=20),\n",
    "    entities=EntitiesOptions(\n",
    "        sentiment=False,\n",
    "            emotion=False,\n",
    "            limit=50),\n",
    "    semantic_roles=SemanticRolesOptions()\n",
    "  ))\n",
    "entities = response['entities']\n",
    "keywords = response['keywords']\n",
    "semantic = response['semantic_roles']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "company\n",
      "Chinese oil company\n",
      "trillion-dollar company\n",
      "stellar quarterly earnings\n",
      "market value\n",
      "highly profitable company\n",
      "competent management team\n",
      "cofounder Steve Jobs\n",
      "enviable global footprint\n",
      "dramatic new device\n",
      "unique design quirks\n",
      "Amazon Web Services\n",
      "Apple Watch\n",
      "Apple devices\n",
      "Apple’s dominance\n",
      "Apple’s business\n",
      "Apple’s revenue\n",
      "prior company\n",
      "big numbers\n",
      "['Apple', 'company', 'Chinese oil company', 'trillion-dollar company', 'stellar quarterly earnings', 'market value', 'highly profitable company', 'competent management team', 'cofounder Steve Jobs', 'enviable global footprint', 'dramatic new device', 'unique design quirks', 'Amazon Web Services', 'Apple Watch', 'Apple devices', 'Apple’s dominance', 'Apple’s business', 'Apple’s revenue', 'prior company', 'big numbers']\n"
     ]
    }
   ],
   "source": [
    "keyString = []\n",
    "for key in keywords:\n",
    "    keyString.append(key['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[5]:\n",
    "\n",
    "\n",
    "# Question 1\n",
    "# Extract keywords and entities\n",
    "# define type of words\n",
    "# create questions\n",
    "def Q1(x):\n",
    "    return {\n",
    "        'Person': \"Who is \",\n",
    "        'Location': \"Where is \"\n",
    "    }.get(x, \"What is \")\n",
    "\n",
    "with open(\"Questions1.txt\", \"w\") as file:\n",
    "    for en in entities:\n",
    "        text = Q1(en['type']) + en['text'] + \"?\"\n",
    "        matches = grammar_tool.check(text)\n",
    "        correct_text = language_check.correct(text, matches)\n",
    "        file.write(\"%s\\n\" % correct_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[6]:\n",
    "\n",
    "\n",
    "#Question 2\n",
    "# Extract keywords and entities\n",
    "# define plurality of keywords using nltk\n",
    "# create question What are? Who are? \n",
    "# TODO. How to determine plurality of word?\n",
    "# TODO. If condition does not suffice, what else?\n",
    "\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def isplural(word):\n",
    "    lemma = wnl.lemmatize(word, 'n')\n",
    "    plural = True if word is not lemma else False\n",
    "    return plural\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "Apple\n",
      "PetroChina, the state-owned Chinese oil company,\n",
      "Apple\n",
      "Apple\n",
      "the Apple Watch\n",
      "Apple\n",
      "by a highly competent management team\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "# Question 3\n",
    "# How would you compare A to B?\n",
    "# For each keyword, look for other words that have same type and relatively high relevance in DB of news articles???\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "# Question 4\n",
    "# Why (factual statement)?\n",
    "# TODO. How to determine which sentences are important enough to negate? TEXTSUM, sumy NOTWORKING\n",
    "\n",
    "# Question 5\n",
    "# What if (negated statements)?\n",
    "# TODO. How to negate sentences? add not, find antonym\n",
    "# TODO. How to determine which sentences are important enough to negate? summarize? gensim, pyteaser, pytextrank, TEXTSUM, sumy\n",
    "\n",
    "\n",
    "# LANGUAGE = \"english\"\n",
    "# SENTENCES_COUNT = 6\n",
    "# if __name__ == \"__main__\":\n",
    "#     #url = \"https://en.wikipedia.org/wiki/Steven_Spielberg\"\n",
    "#     #parser = HtmlParser.from_url(url, Tokenizer(LANGUAGE))\n",
    "#     # or for plain text files\n",
    "#     parser = PlaintextParser.from_file(\"document.txt\", Tokenizer(LANGUAGE))\n",
    "#     stemmer = Stemmer(LANGUAGE)\n",
    "\n",
    "#     summarizer = Summarizer(stemmer)\n",
    "#     summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "#     for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "#         result = natural_language_understanding.analyze(text=str(sentence), features=Features(semantic_roles=SemanticRolesOptions()))\n",
    "#         #print(json.dumps(result, indent=2))\n",
    "#         for sentence in result['semantic_roles']:\n",
    "#             print(len(sentence))\n",
    "#             if len(sentence) == 4:\n",
    "#                 print(\"Why Does\", sentence['subject']['text'], verb, obj)\n",
    "#                 print(\"What if\", sentence['subject']['text'], \"did not\",verb, obj)\n",
    "\n",
    "\n",
    "\n",
    "Q4 = []\n",
    "Q5 = []\n",
    "\n",
    "listOfPlurals = [\"they\", \"some\", \"most\", \"we\"]\n",
    "\n",
    "for sentence in semantic:\n",
    "    if len(sentence) == 4:\n",
    "        verb = sentence['action']['normalized']\n",
    "        subj = sentence['subject']['text']\n",
    "        obj = sentence['object']['text']\n",
    "        hasKeyword = any(key in subj for key in keyString) or any(key in obj for key in keyString)\n",
    "        if len(subj) > 3 and verb is not \"s\" and verb != \"be\" and hasKeyword:\n",
    "            plurality = subj is not inflection.singularize(subj) or subj in listOfPlurals\n",
    "            print(subj)\n",
    "            if plurality:\n",
    "                Q4.append(\"Why do \" + subj + \" \" + verb + \" \" + obj + \"?\")\n",
    "            else:\n",
    "                Q4.append(\"Why does \" + subj + \" \" + verb + \" \" + obj + \"?\")\n",
    "            Q5.append(\"What if \" + subj + \" did not \" + verb + \" \" + obj + \"?\")\n",
    "        elif len(subj) > 3 and verb == \"be\" and hasKeyword:\n",
    "            plurality = subj is not inflection.singularize(subj) or subj in listOfPlurals\n",
    "            if plurality:\n",
    "                Q4.append(\"Why are \" + subj + \" \" + obj + \"?\")\n",
    "                Q5.append(\"What if \" + subj + \" were not \" + obj + \"?\")\n",
    "            else:\n",
    "                Q4.append(\"Why is \" + subj + \" \" + obj + \"?\")\n",
    "                Q5.append(\"What if \" + subj + \" was not \" + obj + \"?\")\n",
    "        \n",
    "with open(\"Questions4.txt\", \"w\", encoding='UTF-8') as file:\n",
    "    for q in Q4:\n",
    "        matches = grammar_tool.check(q)\n",
    "        corrected_q = language_check.correct(q, matches)\n",
    "        file.write(\"%s\\n\" % corrected_q)\n",
    "with open(\"Questions5.txt\", \"w\", encoding='UTF-8') as file:\n",
    "    for q in Q5:\n",
    "        matches = grammar_tool.check(q)\n",
    "        corrected_q = language_check.correct(q, matches)\n",
    "        file.write(\"%s\\n\" % corrected_q)\n",
    "\n",
    "# TODO: Check relevance to text\n",
    "# TODO: lets do grammar check first and delete or fix the grammar before comparing relevance to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
